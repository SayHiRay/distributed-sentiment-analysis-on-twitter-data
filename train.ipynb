{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer, HashingTF, IDF, CountVectorizer, Normalizer, StringIndexer, Tokenizer\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType, DoubleType, StringType, StructType, StructField\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that to run this notebook up you need to wget your own dataset using:\n",
    "\n",
    "wget http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip\n",
    "\n",
    "and unzip the file into ./sentiment140 subfolder.\n",
    "\n",
    "This dataset is too large to push to the github repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = r\"file:///home/jovyan/repos/distributed-sentiment-analysis-on-twitter-data/sentiment140/training.1600000.processed.noemoticon.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = pyspark.SparkConf().setAll([('spark.executor.memory', '3g'),\n",
    "                                   ('spark.executor.cores', '2'), # 4\n",
    "                                   ('spark.cores.max', '2'), # 4\n",
    "                                   ('spark.driver.memory','4g')])\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"TrainSentimentModel\") \\\n",
    "    .config(conf=conf) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.executor.memory', '3g'),\n",
       " ('spark.app.name', 'TrainSentimentModel'),\n",
       " ('spark.driver.port', '43089'),\n",
       " ('spark.driver.memory', '4g'),\n",
       " ('spark.app.id', 'local-1523988106481'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.executor.cores', '2'),\n",
       " ('spark.driver.host', 'de4f1c03e850'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.cores.max', '2'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the data schema(format/structure) for our twitter data in the csv file\n",
    "training_data_schema = StructType([StructField(\"target\", StringType(), True),\n",
    "                                  StructField(\"id\", StringType(), True),\n",
    "                                  StructField(\"date\", StringType(), True),\n",
    "                                  StructField(\"query\", StringType(), True),\n",
    "                                  StructField(\"user\", StringType(), True),\n",
    "                                  StructField(\"text_raw\", StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = spark.read.csv(\n",
    "    data_file, schema=training_data_schema\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------------------------+--------+---------------+---------------------------------------------------------------------------------------------------------------------+\n",
      "|target|id        |date                        |query   |user           |text_raw                                                                                                             |\n",
      "+------+----------+----------------------------+--------+---------------+---------------------------------------------------------------------------------------------------------------------+\n",
      "|0     |1467810369|Mon Apr 06 22:19:45 PDT 2009|NO_QUERY|_TheSpecialOne_|@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D  |\n",
      "|0     |1467810672|Mon Apr 06 22:19:49 PDT 2009|NO_QUERY|scotthamilton  |is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!      |\n",
      "|0     |1467810917|Mon Apr 06 22:19:53 PDT 2009|NO_QUERY|mattycus       |@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds                            |\n",
      "|0     |1467811184|Mon Apr 06 22:19:57 PDT 2009|NO_QUERY|ElleCTF        |my whole body feels itchy and like its on fire                                                                       |\n",
      "|0     |1467811193|Mon Apr 06 22:19:57 PDT 2009|NO_QUERY|Karoli         |@nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there.       |\n",
      "|0     |1467811372|Mon Apr 06 22:20:00 PDT 2009|NO_QUERY|joy_wolf       |@Kwesidei not the whole crew                                                                                         |\n",
      "|0     |1467811592|Mon Apr 06 22:20:03 PDT 2009|NO_QUERY|mybirch        |Need a hug                                                                                                           |\n",
      "|0     |1467811594|Mon Apr 06 22:20:03 PDT 2009|NO_QUERY|coZZ           |@LOLTrish hey  long time no see! Yes.. Rains a bit ,only a bit  LOL , I'm fine thanks , how's you ?                  |\n",
      "|0     |1467811795|Mon Apr 06 22:20:05 PDT 2009|NO_QUERY|2Hood4Hollywood|@Tatiana_K nope they didn't have it                                                                                  |\n",
      "|0     |1467812025|Mon Apr 06 22:20:09 PDT 2009|NO_QUERY|mimismo        |@twittera que me muera ?                                                                                             |\n",
      "|0     |1467812416|Mon Apr 06 22:20:16 PDT 2009|NO_QUERY|erinx3leannexo |spring break in plain city... it's snowing                                                                           |\n",
      "|0     |1467812579|Mon Apr 06 22:20:17 PDT 2009|NO_QUERY|pardonlauren   |I just re-pierced my ears                                                                                            |\n",
      "|0     |1467812723|Mon Apr 06 22:20:19 PDT 2009|NO_QUERY|TLeC           |@caregiving I couldn't bear to watch it.  And I thought the UA loss was embarrassing . . . . .                       |\n",
      "|0     |1467812771|Mon Apr 06 22:20:19 PDT 2009|NO_QUERY|robrobbierobert|@octolinz16 It it counts, idk why I did either. you never talk to me anymore                                         |\n",
      "|0     |1467812784|Mon Apr 06 22:20:20 PDT 2009|NO_QUERY|bayofwolves    |@smarrison i would've been the first, but i didn't have a gun.    not really though, zac snyder's just a doucheclown.|\n",
      "|0     |1467812799|Mon Apr 06 22:20:20 PDT 2009|NO_QUERY|HairByJess     |@iamjazzyfizzle I wish I got to watch it with you!! I miss you and @iamlilnicki  how was the premiere?!              |\n",
      "|0     |1467812964|Mon Apr 06 22:20:22 PDT 2009|NO_QUERY|lovesongwriter |Hollis' death scene will hurt me severely to watch on film  wry is directors cut not out now?                        |\n",
      "|0     |1467813137|Mon Apr 06 22:20:25 PDT 2009|NO_QUERY|armotley       |about to file taxes                                                                                                  |\n",
      "|0     |1467813579|Mon Apr 06 22:20:31 PDT 2009|NO_QUERY|starkissed     |@LettyA ahh ive always wanted to see rent  love the soundtrack!!                                                     |\n",
      "|0     |1467813782|Mon Apr 06 22:20:34 PDT 2009|NO_QUERY|gi_gi_bee      |@FakerPattyPattz Oh dear. Were you drinking out of the forgotten table drinks?                                       |\n",
      "+------+----------+----------------------------+--------+---------------+---------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_raw.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tok = WordPunctTokenizer()\n",
    "\n",
    "at_user_pat = r'@[A-Za-z0-9_]+'  # r'@[\\w]+'\n",
    "url_pat = r'https?://[^ ]+'  # r'https?:\\/\\/[^\\s]+'\n",
    "www_pat = r'www.[^ ]+'\n",
    "repeating_chars_pat = r'([A-Za-z])\\1+'\n",
    "negations_dic = {\"isn't\":\"is not\", \"aren't\":\"are not\", \"wasn't\":\"was not\", \"weren't\":\"were not\",\n",
    "                \"haven't\":\"have not\",\"hasn't\":\"has not\",\"hadn't\":\"had not\",\"won't\":\"will not\",\n",
    "                \"wouldn't\":\"would not\", \"don't\":\"do not\", \"doesn't\":\"does not\",\"didn't\":\"did not\",\n",
    "                \"can't\":\"can not\",\"couldn't\":\"could not\",\"shouldn't\":\"should not\",\"mightn't\":\"might not\",\n",
    "                \"mustn't\":\"must not\"}\n",
    "neg_pattern = re.compile(r'\\b(' + '|'.join(negations_dic.keys()) + r')\\b')\n",
    "\n",
    "def tweet_cleaner(text):\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    souped = soup.get_text()\n",
    "    try:\n",
    "        bom_removed = souped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        bom_removed = souped\n",
    "    stripped = re.sub(at_user_pat, 'USERNAME', bom_removed)\n",
    "    stripped = re.sub(url_pat, 'URL', stripped)\n",
    "    stripped = re.sub(www_pat, 'URL', stripped)\n",
    "    stripped = re.sub(repeating_chars_pat, r'\\1\\1', stripped)\n",
    "    \n",
    "    lower_case = stripped.lower()\n",
    "    neg_handled = neg_pattern.sub(lambda x: negations_dic[x.group()], lower_case)\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", neg_handled)\n",
    "    # During the letters_only process two lines above, it has created unnecessay white spaces,\n",
    "    # I will tokenize and join together to remove unneccessary white spaces\n",
    "    words = [x for x  in tok.tokenize(letters_only) if len(x) > 1]\n",
    "    return (\" \".join(words)).strip()\n",
    "\n",
    "udf_tweet_cleaner = udf(tweet_cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_preprocessed = df_raw.withColumn(\"text\", udf_tweet_cleaner(col(\"text_raw\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------------------------------------------------------------------------------------------------------------+\n",
      "|target|text_raw                                                                                                             |\n",
      "+------+---------------------------------------------------------------------------------------------------------------------+\n",
      "|0     |@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D  |\n",
      "|0     |is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!      |\n",
      "|0     |@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds                            |\n",
      "|0     |my whole body feels itchy and like its on fire                                                                       |\n",
      "|0     |@nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there.       |\n",
      "|0     |@Kwesidei not the whole crew                                                                                         |\n",
      "|0     |Need a hug                                                                                                           |\n",
      "|0     |@LOLTrish hey  long time no see! Yes.. Rains a bit ,only a bit  LOL , I'm fine thanks , how's you ?                  |\n",
      "|0     |@Tatiana_K nope they didn't have it                                                                                  |\n",
      "|0     |@twittera que me muera ?                                                                                             |\n",
      "|0     |spring break in plain city... it's snowing                                                                           |\n",
      "|0     |I just re-pierced my ears                                                                                            |\n",
      "|0     |@caregiving I couldn't bear to watch it.  And I thought the UA loss was embarrassing . . . . .                       |\n",
      "|0     |@octolinz16 It it counts, idk why I did either. you never talk to me anymore                                         |\n",
      "|0     |@smarrison i would've been the first, but i didn't have a gun.    not really though, zac snyder's just a doucheclown.|\n",
      "|0     |@iamjazzyfizzle I wish I got to watch it with you!! I miss you and @iamlilnicki  how was the premiere?!              |\n",
      "|0     |Hollis' death scene will hurt me severely to watch on film  wry is directors cut not out now?                        |\n",
      "|0     |about to file taxes                                                                                                  |\n",
      "|0     |@LettyA ahh ive always wanted to see rent  love the soundtrack!!                                                     |\n",
      "|0     |@FakerPattyPattz Oh dear. Were you drinking out of the forgotten table drinks?                                       |\n",
      "+------+---------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_preprocessed.select(\"target\", \"text_raw\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------------------------------------------------------------------------------------------------+\n",
      "|target|text                                                                                                     |\n",
      "+------+---------------------------------------------------------------------------------------------------------+\n",
      "|0     |username url aww that bummer you shoulda got david carr of third day to do it                            |\n",
      "|0     |is upset that he can not update his facebook by texting it and might cry as result school today also blah|\n",
      "|0     |username dived many times for the ball managed to save the rest go out of bounds                         |\n",
      "|0     |my whole body feels itchy and like its on fire                                                           |\n",
      "|0     |username no it not behaving at all mad why am here because can not see you all over there                |\n",
      "|0     |username not the whole crew                                                                              |\n",
      "|0     |need hug                                                                                                 |\n",
      "|0     |username hey long time no see yes rains bit only bit lol fine thanks how you                             |\n",
      "|0     |username nope they did not have it                                                                       |\n",
      "|0     |username que me muera                                                                                    |\n",
      "|0     |spring break in plain city it snowing                                                                    |\n",
      "|0     |just re pierced my ears                                                                                  |\n",
      "|0     |username could not bear to watch it and thought the ua loss was embarrassing                             |\n",
      "|0     |username it it counts idk why did either you never talk to me anymore                                    |\n",
      "|0     |username would ve been the first but did not have gun not really though zac snyder just doucheclown      |\n",
      "|0     |username wish got to watch it with you miss you and username how was the premiere                        |\n",
      "|0     |hollis death scene will hurt me severely to watch on film wry is directors cut not out now               |\n",
      "|0     |about to file taxes                                                                                      |\n",
      "|0     |username ahh ive always wanted to see rent love the soundtrack                                           |\n",
      "|0     |username oh dear were you drinking out of the forgotten table drinks                                     |\n",
      "+------+---------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_preprocessed.select(\"target\", \"text\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_preprocessed = text_preprocessed.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_set, val_set, test_set) = text_preprocessed.randomSplit([0.98, 0.01, 0.01], seed = 2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 60 ms, sys: 10 ms, total: 70 ms\n",
      "Wall time: 5min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_set.cache()\n",
    "val_set.cache()\n",
    "test_set.cache()\n",
    "\n",
    "train_set.count()\n",
    "val_set.count()\n",
    "test_set.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HashingTF + IDF + Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.7904\n",
      "ROC-AUC: 0.8615\n",
      "CPU times: user 160 ms, sys: 20 ms, total: 180 ms\n",
      "Wall time: 32.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashtf = HashingTF(numFeatures=2**16, inputCol=\"words\", outputCol='tf')\n",
    "idf = IDF(inputCol='tf', outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "label_stringIdx = StringIndexer(inputCol = \"target\", outputCol = \"label\")\n",
    "lr = LogisticRegression(maxIter=100)\n",
    "pipeline = Pipeline(stages=[tokenizer, hashtf, idf, label_stringIdx, lr])\n",
    "\n",
    "pipelineFit = pipeline.fit(train_set)\n",
    "predictions = pipelineFit.transform(val_set)\n",
    "accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float(val_set.count())\n",
    "roc_auc = evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"Accuracy Score: {0:.4f}\".format(accuracy))\n",
    "print(\"ROC-AUC: {0:.4f}\".format(roc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineFit.save(\"hashtf_idf_lr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.7904\n",
      "ROC-AUC: 0.8615\n",
      "CPU times: user 70 ms, sys: 10 ms, total: 80 ms\n",
      "Wall time: 3.62 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pipelineFit_loaded = PipelineModel.load(\"hashtf_idf_lr\")\n",
    "\n",
    "predictions = pipelineFit_loaded.transform(val_set)\n",
    "accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float(val_set.count())\n",
    "roc_auc = evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"Accuracy Score: {0:.4f}\".format(accuracy))\n",
    "print(\"ROC-AUC: {0:.4f}\".format(roc_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CountVectorizer + IDF + Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.7971\n",
      "ROC-AUC: 0.8662\n",
      "CPU times: user 180 ms, sys: 20 ms, total: 200 ms\n",
      "Wall time: 39.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "cv = CountVectorizer(vocabSize=2**16, inputCol=\"words\", outputCol='cv')\n",
    "idf = IDF(inputCol='cv', outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "label_stringIdx = StringIndexer(inputCol = \"target\", outputCol = \"label\")\n",
    "lr = LogisticRegression(maxIter=100)\n",
    "pipeline = Pipeline(stages=[tokenizer, cv, idf, label_stringIdx, lr])\n",
    "\n",
    "pipelineFit = pipeline.fit(train_set)\n",
    "predictions = pipelineFit.transform(val_set)\n",
    "accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float(val_set.count())\n",
    "roc_auc = evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"Accuracy Score: {0:.4f}\".format(accuracy))\n",
    "print(\"ROC-AUC: {0:.4f}\".format(roc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineFit.save(\"cv_idf_lr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.7971\n",
      "ROC-AUC: 0.8662\n",
      "CPU times: user 70 ms, sys: 0 ns, total: 70 ms\n",
      "Wall time: 2.58 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pipelineFit_loaded = PipelineModel.load(\"cv_idf_lr\")\n",
    "\n",
    "predictions = pipelineFit_loaded.transform(val_set)\n",
    "accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float(val_set.count())\n",
    "roc_auc = evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"Accuracy Score: {0:.4f}\".format(accuracy))\n",
    "print(\"ROC-AUC: {0:.4f}\".format(roc_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using N-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import NGram, VectorAssembler\n",
    "from pyspark.ml.feature import ChiSqSelector\n",
    "\n",
    "def build_trigrams(inputCol=[\"text\",\"target\"], n=3):\n",
    "    tokenizer = [Tokenizer(inputCol=\"text\", outputCol=\"words\")]\n",
    "    ngrams = [\n",
    "        NGram(n=i, inputCol=\"words\", outputCol=\"{0}_grams\".format(i))\n",
    "        for i in range(1, n + 1)\n",
    "    ]\n",
    "\n",
    "    cv = [\n",
    "        CountVectorizer(vocabSize=2**14,inputCol=\"{0}_grams\".format(i),\n",
    "            outputCol=\"{0}_tf\".format(i))\n",
    "        for i in range(1, n + 1)\n",
    "    ]\n",
    "    idf = [IDF(inputCol=\"{0}_tf\".format(i), outputCol=\"{0}_tfidf\".format(i), minDocFreq=5) for i in range(1, n + 1)]\n",
    "\n",
    "    assembler = [VectorAssembler(\n",
    "        inputCols=[\"{0}_tfidf\".format(i) for i in range(1, n + 1)],\n",
    "        outputCol=\"rawFeatures\"\n",
    "    )]\n",
    "    label_stringIdx = [StringIndexer(inputCol = \"target\", outputCol = \"label\")]\n",
    "    selector = [ChiSqSelector(numTopFeatures=2**14,featuresCol='rawFeatures', outputCol=\"features\")]\n",
    "    lr = [LogisticRegression(maxIter=100)]\n",
    "    return Pipeline(stages=tokenizer + ngrams + cv + idf+ assembler + label_stringIdx+selector+lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ngrams_wocs(inputCol=[\"text\",\"target\"], n=3):\n",
    "    tokenizer = [Tokenizer(inputCol=\"text\", outputCol=\"words\")]\n",
    "    ngrams = [\n",
    "        NGram(n=i, inputCol=\"words\", outputCol=\"{0}_grams\".format(i))\n",
    "        for i in range(1, n + 1)\n",
    "    ]\n",
    "\n",
    "    cv = [\n",
    "        CountVectorizer(vocabSize=5460,inputCol=\"{0}_grams\".format(i),\n",
    "            outputCol=\"{0}_tf\".format(i))\n",
    "        for i in range(1, n + 1)\n",
    "    ]\n",
    "    idf = [IDF(inputCol=\"{0}_tf\".format(i), outputCol=\"{0}_tfidf\".format(i), minDocFreq=5) for i in range(1, n + 1)]\n",
    "\n",
    "    assembler = [VectorAssembler(\n",
    "        inputCols=[\"{0}_tfidf\".format(i) for i in range(1, n + 1)],\n",
    "        outputCol=\"features\"\n",
    "    )]\n",
    "    label_stringIdx = [StringIndexer(inputCol = \"target\", outputCol = \"label\")]\n",
    "    lr = [LogisticRegression(maxIter=100)]\n",
    "    return Pipeline(stages=tokenizer + ngrams + cv + idf+ assembler + label_stringIdx+lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.8115\n",
      "ROC-AUC: 0.8879\n",
      "CPU times: user 220 ms, sys: 100 ms, total: 320 ms\n",
      "Wall time: 3min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trigramwocs_pipelineFit = build_ngrams_wocs().fit(train_set)\n",
    "predictions_wocs = trigramwocs_pipelineFit.transform(val_set)\n",
    "accuracy_wocs = predictions_wocs.filter(predictions_wocs.label == predictions_wocs.prediction).count() / float(val_set.count())\n",
    "roc_auc_wocs = evaluator.evaluate(predictions_wocs)\n",
    "\n",
    "print(\"Accuracy Score: {0:.4f}\".format(accuracy_wocs))\n",
    "print(\"ROC-AUC: {0:.4f}\".format(roc_auc_wocs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigramwocs_pipelineFit.save(\"ngram_cv_idf_lr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.8115\n",
      "ROC-AUC: 0.8879\n",
      "CPU times: user 130 ms, sys: 20 ms, total: 150 ms\n",
      "Wall time: 2.94 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trigramwocs_pipelineFit_loaded = PipelineModel.load(\"ngram_cv_idf_lr\")\n",
    "\n",
    "predictions = trigramwocs_pipelineFit_loaded.transform(val_set)\n",
    "accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float(val_set.count())\n",
    "roc_auc = evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"Accuracy Score: {0:.4f}\".format(accuracy))\n",
    "print(\"ROC-AUC: {0:.4f}\".format(roc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.8092\n",
      "ROC-AUC: 0.8858\n",
      "CPU times: user 40 ms, sys: 30 ms, total: 70 ms\n",
      "Wall time: 1.41 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_predictions = trigramwocs_pipelineFit.transform(test_set)\n",
    "test_accuracy = test_predictions.filter(test_predictions.label == test_predictions.prediction).count() / float(test_set.count())\n",
    "test_roc_auc = evaluator.evaluate(test_predictions)\n",
    "\n",
    "print(\"Accuracy Score: {0:.4f}\".format(test_accuracy))\n",
    "print(\"ROC-AUC: {0:.4f}\".format(test_roc_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Naive Bayes with ngram_cv_idf_nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ngrams_nb(inputCol=[\"text\",\"target\"], n=3):\n",
    "    tokenizer = [Tokenizer(inputCol=\"text\", outputCol=\"words\")]\n",
    "    ngrams = [\n",
    "        NGram(n=i, inputCol=\"words\", outputCol=\"{0}_grams\".format(i))\n",
    "        for i in range(1, n + 1)\n",
    "    ]\n",
    "\n",
    "    cv = [\n",
    "        CountVectorizer(vocabSize=5460,inputCol=\"{0}_grams\".format(i),\n",
    "            outputCol=\"{0}_tf\".format(i))\n",
    "        for i in range(1, n + 1)\n",
    "    ]\n",
    "    idf = [IDF(inputCol=\"{0}_tf\".format(i), outputCol=\"{0}_tfidf\".format(i), minDocFreq=5) for i in range(1, n + 1)]\n",
    "\n",
    "    assembler = [VectorAssembler(\n",
    "        inputCols=[\"{0}_tfidf\".format(i) for i in range(1, n + 1)],\n",
    "        outputCol=\"features\"\n",
    "    )]\n",
    "    label_stringIdx = [StringIndexer(inputCol = \"target\", outputCol = \"label\")]\n",
    "    nb = [NaiveBayes(smoothing=1.0, modelType=\"multinomial\")]\n",
    "    return Pipeline(stages=tokenizer + ngrams + cv + idf+ assembler + label_stringIdx+nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.7846\n",
      "CPU times: user 270 ms, sys: 70 ms, total: 340 ms\n",
      "Wall time: 2min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "nb_pipelineFit = build_ngrams_nb().fit(train_set)\n",
    "predictions = nb_pipelineFit.transform(val_set)\n",
    "accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float(val_set.count())\n",
    "roc_auc = evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"Accuracy Score: {0:.4f}\".format(accuracy))\n",
    "print(\"ROC-AUC: {0:.4f}\".format(roc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_pipelineFit.save(\"ngram_cv_idf_nb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.7846\n",
      "ROC-AUC: 0.5529\n",
      "CPU times: user 120 ms, sys: 20 ms, total: 140 ms\n",
      "Wall time: 2.74 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "nb_pipelineFit_loaded = PipelineModel.load(\"ngram_cv_idf_nb\")\n",
    "\n",
    "predictions = nb_pipelineFit_loaded.transform(val_set)\n",
    "accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float(val_set.count())\n",
    "roc_auc = evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"Accuracy Score: {0:.4f}\".format(accuracy))\n",
    "print(\"ROC-AUC: {0:.4f}\".format(roc_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Gradient Boost Tree with ngram_cv_idf_gbt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ngrams_gbt(inputCol=[\"text\",\"target\"], n=3):\n",
    "    tokenizer = [Tokenizer(inputCol=\"text\", outputCol=\"words\")]\n",
    "    ngrams = [\n",
    "        NGram(n=i, inputCol=\"words\", outputCol=\"{0}_grams\".format(i))\n",
    "        for i in range(1, n + 1)\n",
    "    ]\n",
    "\n",
    "    cv = [\n",
    "        CountVectorizer(vocabSize=5460,inputCol=\"{0}_grams\".format(i),\n",
    "            outputCol=\"{0}_tf\".format(i))\n",
    "        for i in range(1, n + 1)\n",
    "    ]\n",
    "    idf = [IDF(inputCol=\"{0}_tf\".format(i), outputCol=\"{0}_tfidf\".format(i), minDocFreq=5) for i in range(1, n + 1)]\n",
    "\n",
    "    assembler = [VectorAssembler(\n",
    "        inputCols=[\"{0}_tfidf\".format(i) for i in range(1, n + 1)],\n",
    "        outputCol=\"features\"\n",
    "    )]\n",
    "    label_stringIdx = [StringIndexer(inputCol = \"target\", outputCol = \"label\")]\n",
    "    gbt = [GBTClassifier(labelCol=\"label\", featuresCol=\"features\", maxIter=10)]\n",
    "    return Pipeline(stages=tokenizer + ngrams + cv + idf+ assembler + label_stringIdx+gbt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gbt_pipelineFit = build_ngrams_gbt().fit(train_set)\n",
    "predictions = gbt_pipelineFit.transform(val_set)\n",
    "accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float(val_set.count())\n",
    "roc_auc = evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"Accuracy Score: {0:.4f}\".format(accuracy))\n",
    "print(\"ROC-AUC: {0:.4f}\".format(roc_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ngrams_lsvc(inputCol=[\"text\",\"target\"], n=3):\n",
    "    tokenizer = [Tokenizer(inputCol=\"text\", outputCol=\"words\")]\n",
    "    ngrams = [\n",
    "        NGram(n=i, inputCol=\"words\", outputCol=\"{0}_grams\".format(i))\n",
    "        for i in range(1, n + 1)\n",
    "    ]\n",
    "\n",
    "    cv = [\n",
    "        CountVectorizer(vocabSize=5460,inputCol=\"{0}_grams\".format(i),\n",
    "            outputCol=\"{0}_tf\".format(i))\n",
    "        for i in range(1, n + 1)\n",
    "    ]\n",
    "    idf = [IDF(inputCol=\"{0}_tf\".format(i), outputCol=\"{0}_tfidf\".format(i), minDocFreq=5) for i in range(1, n + 1)]\n",
    "\n",
    "    assembler = [VectorAssembler(\n",
    "        inputCols=[\"{0}_tfidf\".format(i) for i in range(1, n + 1)],\n",
    "        outputCol=\"features\"\n",
    "    )]\n",
    "    label_stringIdx = [StringIndexer(inputCol = \"target\", outputCol = \"label\")]\n",
    "    lsvc = [LinearSVC(maxIter=10, regParam=0.1)]\n",
    "    return Pipeline(stages=tokenizer + ngrams + cv + idf+ assembler + label_stringIdx+lsvc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.8080\n",
      "ROC-AUC: 0.8840\n",
      "CPU times: user 250 ms, sys: 110 ms, total: 360 ms\n",
      "Wall time: 12min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lsvc_pipelineFit = build_ngrams_lsvc().fit(train_set)\n",
    "predictions = lsvc_pipelineFit.transform(val_set)\n",
    "accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float(val_set.count())\n",
    "roc_auc = evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"Accuracy Score: {0:.4f}\".format(accuracy))\n",
    "print(\"ROC-AUC: {0:.4f}\".format(roc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsvc_pipelineFit.save(\"ngram_cv_idf_lsvc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.8080\n",
      "ROC-AUC: 0.8840\n",
      "CPU times: user 90 ms, sys: 40 ms, total: 130 ms\n",
      "Wall time: 3.69 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lsvc_pipelineFit_loaded = PipelineModel.load(\"ngram_cv_idf_lsvc\")\n",
    "\n",
    "predictions = lsvc_pipelineFit_loaded.transform(val_set)\n",
    "accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float(val_set.count())\n",
    "roc_auc = evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"Accuracy Score: {0:.4f}\".format(accuracy))\n",
    "print(\"ROC-AUC: {0:.4f}\".format(roc_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Performance of Models on Sentiment140 Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_file = r\"file:///home/jovyan/repos/distributed-sentiment-analysis-on-twitter-data/sentiment140/testdata.manual.2009.06.14.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_schema = StructType([StructField(\"target\", StringType(), True),\n",
    "                               StructField(\"id\", StringType(), True),\n",
    "                               StructField(\"date\", StringType(), True),\n",
    "                               StructField(\"query\", StringType(), True),\n",
    "                               StructField(\"user\", StringType(), True),\n",
    "                               StructField(\"text_raw\", StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = spark.read.csv(\n",
    "    test_data_file, schema=test_data_schema\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+--------------------+-------+--------+--------------------+\n",
      "|target| id|                date|  query|    user|            text_raw|\n",
      "+------+---+--------------------+-------+--------+--------------------+\n",
      "|     4|  3|Mon May 11 03:17:...|kindle2|  tpryan|@stellargirl I lo...|\n",
      "|     4|  4|Mon May 11 03:18:...|kindle2|  vcu451|Reading my kindle...|\n",
      "|     4|  5|Mon May 11 03:18:...|kindle2|  chadfu|Ok, first assesme...|\n",
      "|     4|  6|Mon May 11 03:19:...|kindle2|   SIX15|@kenburbary You'l...|\n",
      "|     4|  7|Mon May 11 03:21:...|kindle2|yamarama|@mikefish  Fair e...|\n",
      "+------+---+--------------------+-------+--------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_set = df_test.withColumn(\"text\", udf_tweet_cleaner(col(\"text_raw\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_set = benchmark_set.filter((col(\"target\") != \"2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "359"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark_set.cache()\n",
    "\n",
    "benchmark_set.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing hashtf_idf_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineFit_loaded = PipelineModel.load(\"hashtf_idf_lr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.7827\n",
      "ROC-AUC: 0.8436\n",
      "CPU times: user 10 ms, sys: 20 ms, total: 30 ms\n",
      "Wall time: 288 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "predictions = pipelineFit_loaded.transform(benchmark_set)\n",
    "accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float(benchmark_set.count())\n",
    "roc_auc = evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"Accuracy Score: {0:.4f}\".format(accuracy))\n",
    "print(\"ROC-AUC: {0:.4f}\".format(roc_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing cv_idf_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineFit_loaded = PipelineModel.load(\"cv_idf_lr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.7827\n",
      "ROC-AUC: 0.8446\n",
      "CPU times: user 30 ms, sys: 0 ns, total: 30 ms\n",
      "Wall time: 851 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "predictions = pipelineFit_loaded.transform(benchmark_set)\n",
    "accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float(benchmark_set.count())\n",
    "roc_auc = evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"Accuracy Score: {0:.4f}\".format(accuracy))\n",
    "print(\"ROC-AUC: {0:.4f}\".format(roc_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing ngram_cv_idf_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigramwocs_pipelineFit_loaded = PipelineModel.load(\"ngram_cv_idf_lr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.8106\n",
      "ROC-AUC: 0.9019\n",
      "CPU times: user 40 ms, sys: 30 ms, total: 70 ms\n",
      "Wall time: 634 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "test_predictions = trigramwocs_pipelineFit_loaded.transform(benchmark_set)\n",
    "test_accuracy = test_predictions.filter(test_predictions.label == test_predictions.prediction).count() / float(benchmark_set.count())\n",
    "test_roc_auc = evaluator.evaluate(test_predictions)\n",
    "\n",
    "print(\"Accuracy Score: {0:.4f}\".format(test_accuracy))\n",
    "print(\"ROC-AUC: {0:.4f}\".format(test_roc_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing NLTK Vader Analyser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "vader_analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_sentiment_analysis(text):\n",
    "    result = vader_analyzer.polarity_scores(str(text))\n",
    "    prediction = \"4\" if result['compound'] >= 0 else \"0\"\n",
    "    # prediction = \"4\" if result['pos'] >= result['neg'] else \"0\"\n",
    "\n",
    "    return prediction\n",
    "\n",
    "udf_nltk_sentiment_analysis = udf(nltk_sentiment_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.7855\n",
      "CPU times: user 10 ms, sys: 0 ns, total: 10 ms\n",
      "Wall time: 220 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "test_predictions = benchmark_set.withColumn(\"nltk_prediction\", udf_nltk_sentiment_analysis(col(\"text\")))\n",
    "test_accuracy = test_predictions.filter(test_predictions.target == test_predictions.nltk_prediction).count() / float(benchmark_set.count())\n",
    "\n",
    "print(\"Accuracy Score: {0:.4f}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
