{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer, HashingTF, IDF, CountVectorizer, Normalizer, StringIndexer, Tokenizer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType, DoubleType, StringType, StructType, StructField\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that to run this notebook up you need to wget your own dataset using:\n",
    "\n",
    "wget http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip\n",
    "\n",
    "and unzip the file into ./sentiment140 subfolder.\n",
    "\n",
    "This dataset is too large to push to the github repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = r\"file:///home/jovyan/repos/distributed-sentiment-analysis-on-twitter-data/sentiment140/training.1600000.processed.noemoticon.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = pyspark.SparkConf().setAll([('spark.executor.memory', '6g'),\n",
    "                                   ('spark.executor.cores', '3'),\n",
    "                                   ('spark.cores.max', '3'),\n",
    "                                   ('spark.driver.memory','2g')])\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"TrainSentimentModel\") \\\n",
    "    .config(conf=conf) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.executor.memory', '2g'),\n",
       " ('spark.app.name', 'TrainSentimentModel'),\n",
       " ('spark.app.id', 'local-1523945709991'),\n",
       " ('spark.driver.port', '43687'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.cores.max', '3'),\n",
       " ('spark.driver.host', 'de4f1c03e850'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.driver.memory', '2g'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.executor.cores', '3'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the data schema(format/structure) for our twitter data in the csv file\n",
    "training_data_schema = StructType([StructField(\"target\", StringType(), True),\n",
    "                                  StructField(\"id\", StringType(), True),\n",
    "                                  StructField(\"date\", StringType(), True),\n",
    "                                  StructField(\"query\", StringType(), True),\n",
    "                                  StructField(\"user\", StringType(), True),\n",
    "                                  StructField(\"text_raw\", StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = spark.read.csv(\n",
    "    data_file, schema=training_data_schema\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------------------------+--------+---------------+---------------------------------------------------------------------------------------------------------------------+\n",
      "|target|id        |date                        |query   |user           |text_raw                                                                                                             |\n",
      "+------+----------+----------------------------+--------+---------------+---------------------------------------------------------------------------------------------------------------------+\n",
      "|0     |1467810369|Mon Apr 06 22:19:45 PDT 2009|NO_QUERY|_TheSpecialOne_|@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D  |\n",
      "|0     |1467810672|Mon Apr 06 22:19:49 PDT 2009|NO_QUERY|scotthamilton  |is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!      |\n",
      "|0     |1467810917|Mon Apr 06 22:19:53 PDT 2009|NO_QUERY|mattycus       |@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds                            |\n",
      "|0     |1467811184|Mon Apr 06 22:19:57 PDT 2009|NO_QUERY|ElleCTF        |my whole body feels itchy and like its on fire                                                                       |\n",
      "|0     |1467811193|Mon Apr 06 22:19:57 PDT 2009|NO_QUERY|Karoli         |@nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there.       |\n",
      "|0     |1467811372|Mon Apr 06 22:20:00 PDT 2009|NO_QUERY|joy_wolf       |@Kwesidei not the whole crew                                                                                         |\n",
      "|0     |1467811592|Mon Apr 06 22:20:03 PDT 2009|NO_QUERY|mybirch        |Need a hug                                                                                                           |\n",
      "|0     |1467811594|Mon Apr 06 22:20:03 PDT 2009|NO_QUERY|coZZ           |@LOLTrish hey  long time no see! Yes.. Rains a bit ,only a bit  LOL , I'm fine thanks , how's you ?                  |\n",
      "|0     |1467811795|Mon Apr 06 22:20:05 PDT 2009|NO_QUERY|2Hood4Hollywood|@Tatiana_K nope they didn't have it                                                                                  |\n",
      "|0     |1467812025|Mon Apr 06 22:20:09 PDT 2009|NO_QUERY|mimismo        |@twittera que me muera ?                                                                                             |\n",
      "|0     |1467812416|Mon Apr 06 22:20:16 PDT 2009|NO_QUERY|erinx3leannexo |spring break in plain city... it's snowing                                                                           |\n",
      "|0     |1467812579|Mon Apr 06 22:20:17 PDT 2009|NO_QUERY|pardonlauren   |I just re-pierced my ears                                                                                            |\n",
      "|0     |1467812723|Mon Apr 06 22:20:19 PDT 2009|NO_QUERY|TLeC           |@caregiving I couldn't bear to watch it.  And I thought the UA loss was embarrassing . . . . .                       |\n",
      "|0     |1467812771|Mon Apr 06 22:20:19 PDT 2009|NO_QUERY|robrobbierobert|@octolinz16 It it counts, idk why I did either. you never talk to me anymore                                         |\n",
      "|0     |1467812784|Mon Apr 06 22:20:20 PDT 2009|NO_QUERY|bayofwolves    |@smarrison i would've been the first, but i didn't have a gun.    not really though, zac snyder's just a doucheclown.|\n",
      "|0     |1467812799|Mon Apr 06 22:20:20 PDT 2009|NO_QUERY|HairByJess     |@iamjazzyfizzle I wish I got to watch it with you!! I miss you and @iamlilnicki  how was the premiere?!              |\n",
      "|0     |1467812964|Mon Apr 06 22:20:22 PDT 2009|NO_QUERY|lovesongwriter |Hollis' death scene will hurt me severely to watch on film  wry is directors cut not out now?                        |\n",
      "|0     |1467813137|Mon Apr 06 22:20:25 PDT 2009|NO_QUERY|armotley       |about to file taxes                                                                                                  |\n",
      "|0     |1467813579|Mon Apr 06 22:20:31 PDT 2009|NO_QUERY|starkissed     |@LettyA ahh ive always wanted to see rent  love the soundtrack!!                                                     |\n",
      "|0     |1467813782|Mon Apr 06 22:20:34 PDT 2009|NO_QUERY|gi_gi_bee      |@FakerPattyPattz Oh dear. Were you drinking out of the forgotten table drinks?                                       |\n",
      "+------+----------+----------------------------+--------+---------------+---------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_raw.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tok = WordPunctTokenizer()\n",
    "\n",
    "at_user_pat = r'@[A-Za-z0-9_]+'  # r'@[\\w]+'\n",
    "url_pat = r'https?://[^ ]+'  # r'https?:\\/\\/[^\\s]+'\n",
    "www_pat = r'www.[^ ]+'\n",
    "repeating_chars_pat = r'([A-Za-z])\\1+'\n",
    "negations_dic = {\"isn't\":\"is not\", \"aren't\":\"are not\", \"wasn't\":\"was not\", \"weren't\":\"were not\",\n",
    "                \"haven't\":\"have not\",\"hasn't\":\"has not\",\"hadn't\":\"had not\",\"won't\":\"will not\",\n",
    "                \"wouldn't\":\"would not\", \"don't\":\"do not\", \"doesn't\":\"does not\",\"didn't\":\"did not\",\n",
    "                \"can't\":\"can not\",\"couldn't\":\"could not\",\"shouldn't\":\"should not\",\"mightn't\":\"might not\",\n",
    "                \"mustn't\":\"must not\"}\n",
    "neg_pattern = re.compile(r'\\b(' + '|'.join(negations_dic.keys()) + r')\\b')\n",
    "\n",
    "def tweet_cleaner(text):\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    souped = soup.get_text()\n",
    "    try:\n",
    "        bom_removed = souped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        bom_removed = souped\n",
    "    stripped = re.sub(at_user_pat, 'USERNAME', bom_removed)\n",
    "    stripped = re.sub(url_pat, 'URL', stripped)\n",
    "    stripped = re.sub(www_pat, 'URL', stripped)\n",
    "    stripped = re.sub(repeating_chars_pat, r'\\1\\1', stripped)\n",
    "    \n",
    "    lower_case = stripped.lower()\n",
    "    neg_handled = neg_pattern.sub(lambda x: negations_dic[x.group()], lower_case)\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", neg_handled)\n",
    "    # During the letters_only process two lines above, it has created unnecessay white spaces,\n",
    "    # I will tokenize and join together to remove unneccessary white spaces\n",
    "    words = [x for x  in tok.tokenize(letters_only) if len(x) > 1]\n",
    "    return (\" \".join(words)).strip()\n",
    "\n",
    "udf_tweet_cleaner = udf(tweet_cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_preprocessed = df_raw.withColumn(\"text\", udf_tweet_cleaner(col(\"text_raw\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------------------------------------------------------------------------------------------------------------+\n",
      "|target|text_raw                                                                                                             |\n",
      "+------+---------------------------------------------------------------------------------------------------------------------+\n",
      "|0     |@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D  |\n",
      "|0     |is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!      |\n",
      "|0     |@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds                            |\n",
      "|0     |my whole body feels itchy and like its on fire                                                                       |\n",
      "|0     |@nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there.       |\n",
      "|0     |@Kwesidei not the whole crew                                                                                         |\n",
      "|0     |Need a hug                                                                                                           |\n",
      "|0     |@LOLTrish hey  long time no see! Yes.. Rains a bit ,only a bit  LOL , I'm fine thanks , how's you ?                  |\n",
      "|0     |@Tatiana_K nope they didn't have it                                                                                  |\n",
      "|0     |@twittera que me muera ?                                                                                             |\n",
      "|0     |spring break in plain city... it's snowing                                                                           |\n",
      "|0     |I just re-pierced my ears                                                                                            |\n",
      "|0     |@caregiving I couldn't bear to watch it.  And I thought the UA loss was embarrassing . . . . .                       |\n",
      "|0     |@octolinz16 It it counts, idk why I did either. you never talk to me anymore                                         |\n",
      "|0     |@smarrison i would've been the first, but i didn't have a gun.    not really though, zac snyder's just a doucheclown.|\n",
      "|0     |@iamjazzyfizzle I wish I got to watch it with you!! I miss you and @iamlilnicki  how was the premiere?!              |\n",
      "|0     |Hollis' death scene will hurt me severely to watch on film  wry is directors cut not out now?                        |\n",
      "|0     |about to file taxes                                                                                                  |\n",
      "|0     |@LettyA ahh ive always wanted to see rent  love the soundtrack!!                                                     |\n",
      "|0     |@FakerPattyPattz Oh dear. Were you drinking out of the forgotten table drinks?                                       |\n",
      "+------+---------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_preprocessed.select(\"target\", \"text_raw\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------------------------------------------------------------------------------------------------+\n",
      "|target|text                                                                                                     |\n",
      "+------+---------------------------------------------------------------------------------------------------------+\n",
      "|0     |username url aww that bummer you shoulda got david carr of third day to do it                            |\n",
      "|0     |is upset that he can not update his facebook by texting it and might cry as result school today also blah|\n",
      "|0     |username dived many times for the ball managed to save the rest go out of bounds                         |\n",
      "|0     |my whole body feels itchy and like its on fire                                                           |\n",
      "|0     |username no it not behaving at all mad why am here because can not see you all over there                |\n",
      "|0     |username not the whole crew                                                                              |\n",
      "|0     |need hug                                                                                                 |\n",
      "|0     |username hey long time no see yes rains bit only bit lol fine thanks how you                             |\n",
      "|0     |username nope they did not have it                                                                       |\n",
      "|0     |username que me muera                                                                                    |\n",
      "|0     |spring break in plain city it snowing                                                                    |\n",
      "|0     |just re pierced my ears                                                                                  |\n",
      "|0     |username could not bear to watch it and thought the ua loss was embarrassing                             |\n",
      "|0     |username it it counts idk why did either you never talk to me anymore                                    |\n",
      "|0     |username would ve been the first but did not have gun not really though zac snyder just doucheclown      |\n",
      "|0     |username wish got to watch it with you miss you and username how was the premiere                        |\n",
      "|0     |hollis death scene will hurt me severely to watch on film wry is directors cut not out now               |\n",
      "|0     |about to file taxes                                                                                      |\n",
      "|0     |username ahh ive always wanted to see rent love the soundtrack                                           |\n",
      "|0     |username oh dear were you drinking out of the forgotten table drinks                                     |\n",
      "+------+---------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_preprocessed.select(\"target\", \"text\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_preprocessed = text_preprocessed.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1600000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_preprocessed.count()  # due to lazy execution, this takes a while to run. A minute or two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_set, val_set, test_set) = text_preprocessed.randomSplit([0.98, 0.01, 0.01], seed = 2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HashingTF + IDF + Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------------------+--------+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|target|        id|                date|   query|           user|            text_raw|                text|               words|                  tf|            features|label|\n",
      "+------+----------+--------------------+--------+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|     0|1467810369|Mon Apr 06 22:19:...|NO_QUERY|_TheSpecialOne_|@switchfoot http:...|username url aww ...|[username, url, a...|(65536,[8436,8847...|(65536,[8436,8847...|  0.0|\n",
      "|     0|1467810672|Mon Apr 06 22:19:...|NO_QUERY|  scotthamilton|is upset that he ...|is upset that he ...|[is, upset, that,...|(65536,[1444,2071...|(65536,[1444,2071...|  0.0|\n",
      "|     0|1467810917|Mon Apr 06 22:19:...|NO_QUERY|       mattycus|@Kenichan I dived...|username dived ma...|[username, dived,...|(65536,[2548,2888...|(65536,[2548,2888...|  0.0|\n",
      "|     0|1467811184|Mon Apr 06 22:19:...|NO_QUERY|        ElleCTF|my whole body fee...|my whole body fee...|[my, whole, body,...|(65536,[158,11650...|(65536,[158,11650...|  0.0|\n",
      "|     0|1467811193|Mon Apr 06 22:19:...|NO_QUERY|         Karoli|@nationwideclass ...|username no it no...|[username, no, it...|(65536,[1968,4488...|(65536,[1968,4488...|  0.0|\n",
      "+------+----------+--------------------+--------+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashtf = HashingTF(numFeatures=2**16, inputCol=\"words\", outputCol='tf')\n",
    "idf = IDF(inputCol='tf', outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "label_stringIdx = StringIndexer(inputCol = \"target\", outputCol = \"label\")\n",
    "pipeline = Pipeline(stages=[tokenizer, hashtf, idf, label_stringIdx])\n",
    "\n",
    "pipelineFit = pipeline.fit(train_set)\n",
    "train_df = pipelineFit.transform(train_set)\n",
    "val_df = pipelineFit.transform(val_set)\n",
    "train_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC: 0.8630617822217924\n",
      "Accuracy: 0.7927058528637674\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(maxIter=100)\n",
    "lrModel = lr.fit(train_df)\n",
    "predictions = lrModel.transform(val_df)\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "auroc = evaluator.evaluate(predictions)\n",
    "accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float(val_set.count())\n",
    "\n",
    "print(\"AUROC: {}\\nAccuracy: {}\".format(auroc, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CountVectorizer + IDF + Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "cv = CountVectorizer(vocabSize=2**16, inputCol=\"words\", outputCol='cv')\n",
    "idf = IDF(inputCol='cv', outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "label_stringIdx = StringIndexer(inputCol = \"target\", outputCol = \"label\")\n",
    "lr = LogisticRegression(maxIter=100)\n",
    "pipeline = Pipeline(stages=[tokenizer, cv, idf, label_stringIdx, lr])\n",
    "\n",
    "pipelineFit = pipeline.fit(train_set)\n",
    "predictions = pipelineFit.transform(val_set)\n",
    "accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float(val_set.count())\n",
    "roc_auc = evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"Accuracy Score: {0:.4f}\".format(accuracy))\n",
    "print(\"ROC-AUC: {0:.4f}\".format(roc_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using N-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import NGram, VectorAssembler\n",
    "from pyspark.ml.feature import ChiSqSelector\n",
    "\n",
    "def build_trigrams(inputCol=[\"text\",\"target\"], n=3):\n",
    "    tokenizer = [Tokenizer(inputCol=\"text\", outputCol=\"words\")]\n",
    "    ngrams = [\n",
    "        NGram(n=i, inputCol=\"words\", outputCol=\"{0}_grams\".format(i))\n",
    "        for i in range(1, n + 1)\n",
    "    ]\n",
    "\n",
    "    cv = [\n",
    "        CountVectorizer(vocabSize=2**14,inputCol=\"{0}_grams\".format(i),\n",
    "            outputCol=\"{0}_tf\".format(i))\n",
    "        for i in range(1, n + 1)\n",
    "    ]\n",
    "    idf = [IDF(inputCol=\"{0}_tf\".format(i), outputCol=\"{0}_tfidf\".format(i), minDocFreq=5) for i in range(1, n + 1)]\n",
    "\n",
    "    assembler = [VectorAssembler(\n",
    "        inputCols=[\"{0}_tfidf\".format(i) for i in range(1, n + 1)],\n",
    "        outputCol=\"rawFeatures\"\n",
    "    )]\n",
    "    label_stringIdx = [StringIndexer(inputCol = \"target\", outputCol = \"label\")]\n",
    "    selector = [ChiSqSelector(numTopFeatures=2**14,featuresCol='rawFeatures', outputCol=\"features\")]\n",
    "    lr = [LogisticRegression(maxIter=100)]\n",
    "    return Pipeline(stages=tokenizer + ngrams + cv + idf+ assembler + label_stringIdx+selector+lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ngrams_wocs(inputCol=[\"text\",\"target\"], n=3):\n",
    "    tokenizer = [Tokenizer(inputCol=\"text\", outputCol=\"words\")]\n",
    "    ngrams = [\n",
    "        NGram(n=i, inputCol=\"words\", outputCol=\"{0}_grams\".format(i))\n",
    "        for i in range(1, n + 1)\n",
    "    ]\n",
    "\n",
    "    cv = [\n",
    "        CountVectorizer(vocabSize=5460,inputCol=\"{0}_grams\".format(i),\n",
    "            outputCol=\"{0}_tf\".format(i))\n",
    "        for i in range(1, n + 1)\n",
    "    ]\n",
    "    idf = [IDF(inputCol=\"{0}_tf\".format(i), outputCol=\"{0}_tfidf\".format(i), minDocFreq=5) for i in range(1, n + 1)]\n",
    "\n",
    "    assembler = [VectorAssembler(\n",
    "        inputCols=[\"{0}_tfidf\".format(i) for i in range(1, n + 1)],\n",
    "        outputCol=\"features\"\n",
    "    )]\n",
    "    label_stringIdx = [StringIndexer(inputCol = \"target\", outputCol = \"label\")]\n",
    "    lr = [LogisticRegression(maxIter=100)]\n",
    "    return Pipeline(stages=tokenizer + ngrams + cv + idf+ assembler + label_stringIdx+lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trigramwocs_pipelineFit = build_ngrams_wocs().fit(train_set)\n",
    "predictions_wocs = trigramwocs_pipelineFit.transform(val_set)\n",
    "accuracy_wocs = predictions_wocs.filter(predictions_wocs.label == predictions_wocs.prediction).count() / float(val_set.count())\n",
    "roc_auc_wocs = evaluator.evaluate(predictions_wocs)\n",
    "\n",
    "print(\"Accuracy Score: {0:.4f}\".format(accuracy_wocs))\n",
    "print(\"ROC-AUC: {0:.4f}\".format(roc_auc_wocs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = trigramwocs_pipelineFit.transform(test_set)\n",
    "test_accuracy = test_predictions.filter(test_predictions.label == test_predictions.prediction).count() / float(test_set.count())\n",
    "test_roc_auc = evaluator.evaluate(test_predictions)\n",
    "\n",
    "print(\"Accuracy Score: {0:.4f}\".format(test_accuracy))\n",
    "print(\"ROC-AUC: {0:.4f}\".format(test_roc_auc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
